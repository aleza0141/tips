{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"develop/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"Overview"},{"location":"develop/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"formats/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"File formats"},{"location":"formats/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"library/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"Overview"},{"location":"library/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"cli/bias/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"tips bias"},{"location":"cli/bias/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"cli/convert/","text":"tips convert Convert one or several datasets into another format. See tips.io for the formats available. Usage pinn convert [options] ds1 ds2 ... Options Option [shorthand] Default Description --output [-o] 'dataset' name of the output dataset --fmt [-f] 'auto' format of input dataset --ofmt [-of] 'runner' format of output dataset --emap [-em] None map the elements according to a LAMMPS data file","title":"tips convert"},{"location":"cli/convert/#tips-convert","text":"Convert one or several datasets into another format. See tips.io for the formats available.","title":"tips convert"},{"location":"cli/convert/#usage","text":"pinn convert [options] ds1 ds2 ...","title":"Usage"},{"location":"cli/convert/#options","text":"Option [shorthand] Default Description --output [-o] 'dataset' name of the output dataset --fmt [-f] 'auto' format of input dataset --ofmt [-of] 'runner' format of output dataset --emap [-em] None map the elements according to a LAMMPS data file","title":"Options"},{"location":"cli/subsample/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"tips subsample"},{"location":"cli/subsample/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"cli/wizard/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"tips wizard"},{"location":"cli/wizard/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"dev/dataset/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"Dataset"},{"location":"dev/dataset/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"dev/format/","text":"Adding a dataset format To add a file format, you need to extend the DatasetReader and DatasetWriter class in the tips.io modules. The classes define the availability of labels and how they can be read/written. The DataReader class A DataReader should at least implement a __init__ method that defines data formats given a input dataset , optionally, if the format can have variable units, a unit argument can be supplied: class MyDs ( DatasetReader ): def __init__ ( self , dataset , unit = None ): # comment attributes in `DataReader`s self . indexable = True self . size = None self . ds_spec = {} # reserved for `.iter()` or `.index()` methods self . _dsfile = dataset self . _index = None The indexable and size attribute determines how the dataset might be read, when indexable is False and size is not available, the dataset will only be read iteratively, this makes tasks like splitting the dataset more expansive, since the dataset must be enumerated once before the split can be determined. Whenever possible, it is advisable to implement the .index() function if a specific data point can be retrieved without loading the entire dataset. For plain text formats, this is possible by a fast scan through the file for certain pattern and subsequently rewind the text file with the file.seek() function, an example is the runner.RunnerLoader . The DataWriter class A DataWriter is also initialized with the dataset specifying the output location and the optional unit arugment. The .write() method should write one sample to the dataset, and the .finalize() method to finalize the writing. When implementing the write() method, it is again advisable to avoid caching the entire dataset in the memory. Reserved labels Some generally used label names are hard-coded, which have fixed dimension definitions. Name Shape Type Description coord [natoms, 3] float cartesian coordinates of atoms force [natoms, 3] float atomic forces charge [natoms, 3] float atomic chages elem [natoms] float atomic numbers cell [3, 3] float cell vectors pbc [3] bool periodic boundary condition energy [] float total energy When those data exist in the dataset, they should be named consistently. Checklist Below is a checklist for adding a new file format to the TIPS repo: Implement the format in python/io/your_format.py ; Document the file format in docs/python/io.md ; Adding a unit test case in python/test/io.py ; Submit your pull request! To make a smooth PR, also kindly check the developer setup and contributing guideline.","title":"Format"},{"location":"dev/format/#adding-a-dataset-format","text":"To add a file format, you need to extend the DatasetReader and DatasetWriter class in the tips.io modules. The classes define the availability of labels and how they can be read/written.","title":"Adding a dataset format"},{"location":"dev/format/#the-datareader-class","text":"A DataReader should at least implement a __init__ method that defines data formats given a input dataset , optionally, if the format can have variable units, a unit argument can be supplied: class MyDs ( DatasetReader ): def __init__ ( self , dataset , unit = None ): # comment attributes in `DataReader`s self . indexable = True self . size = None self . ds_spec = {} # reserved for `.iter()` or `.index()` methods self . _dsfile = dataset self . _index = None The indexable and size attribute determines how the dataset might be read, when indexable is False and size is not available, the dataset will only be read iteratively, this makes tasks like splitting the dataset more expansive, since the dataset must be enumerated once before the split can be determined. Whenever possible, it is advisable to implement the .index() function if a specific data point can be retrieved without loading the entire dataset. For plain text formats, this is possible by a fast scan through the file for certain pattern and subsequently rewind the text file with the file.seek() function, an example is the runner.RunnerLoader .","title":"The DataReader class"},{"location":"dev/format/#the-datawriter-class","text":"A DataWriter is also initialized with the dataset specifying the output location and the optional unit arugment. The .write() method should write one sample to the dataset, and the .finalize() method to finalize the writing. When implementing the write() method, it is again advisable to avoid caching the entire dataset in the memory.","title":"The DataWriter class"},{"location":"dev/format/#reserved-labels","text":"Some generally used label names are hard-coded, which have fixed dimension definitions. Name Shape Type Description coord [natoms, 3] float cartesian coordinates of atoms force [natoms, 3] float atomic forces charge [natoms, 3] float atomic chages elem [natoms] float atomic numbers cell [3, 3] float cell vectors pbc [3] bool periodic boundary condition energy [] float total energy When those data exist in the dataset, they should be named consistently.","title":"Reserved labels"},{"location":"dev/format/#checklist","text":"Below is a checklist for adding a new file format to the TIPS repo: Implement the format in python/io/your_format.py ; Document the file format in docs/python/io.md ; Adding a unit test case in python/test/io.py ; Submit your pull request! To make a smooth PR, also kindly check the developer setup and contributing guideline.","title":"Checklist"},{"location":"dev/guideline/","text":"Contributing Guidline Python coding convention The Python module of TIPS follows the black convention.","title":"Guideline"},{"location":"dev/guideline/#contributing-guidline","text":"","title":"Contributing Guidline"},{"location":"dev/guideline/#python-coding-convention","text":"The Python module of TIPS follows the black convention.","title":"Python coding convention"},{"location":"dev/guideline/#_1","text":"","title":""},{"location":"dev/module/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"Module"},{"location":"dev/module/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"dev/pattern/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"Recipie"},{"location":"dev/pattern/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"dev/setup/","text":"Developer setup The page collects information on the instructions to setup a developer's environment to work with TIPS. It is not necessary for TIPS developers to follow these setup, but following this setup helps to maintain a consistent code style and eases the process of pull request. VS Code EMACS","title":"Developer setup"},{"location":"dev/setup/#developer-setup","text":"The page collects information on the instructions to setup a developer's environment to work with TIPS. It is not necessary for TIPS developers to follow these setup, but following this setup helps to maintain a consistent code style and eases the process of pull request.","title":"Developer setup"},{"location":"dev/setup/#vs-code","text":"","title":"VS Code"},{"location":"dev/setup/#emacs","text":"","title":"EMACS"},{"location":"module/ase/","text":"ASE workflows aseMD The aseMD process takes a trained model and runs a MD trajecotry, with some limited options regarding the dynamics. The process suppors PiNN models only for now. A list of models can be supplied to run an ensemble MD. For more complex processes, consider writing a customized MD process and include it in your workflow. Channels Channel Type Note (in) name val an id to identify the process (in) model file trained ANN model, can be a list of models (in) flags val a string specifying the MD simulation see options below (in) init file initial geometry, in any ASE recognizable format (in) pub val path to publish the output trajectory (out) traj file output trajectory (out) log file MD log Options The MD specifics for aseMD is specified as a string of flags in the form of --option1 val1 --option2 val , the available options and default valeus are down below. Option Default Note --ensemble 'nvt' 'npt' or 'nvt' --T 273 Temperature in K --t 100 Time in ps --dt 0.5 Time step in fs --taut 100 Damping factor for thermostat in steps --taup 1000 Damping factor for barostat in steps --log-every 5 Log interval in steps --pressure 1 pressure in bar --compressibility 4.57e-5 compressibility in bar Source code nextflow . enable . dsl = 2 params . publish = 'ase' process aseMD { tag \"$name\" label 'ase' publishDir \"$param.publish/$name\" input: val name path model , stageAs: 'model*' path init val flags output: tuple val ( name ), path ( 'asemd.traj' ), emit: traj tuple val ( name ), path ( 'asemd.log' ), emit: log script: \"\"\" #!/usr/bin/env python import re import pinn import tensorflow as tf from ase import units from ase.io import read from ase.io.trajectory import Trajectory from ase.md import MDLogger from ase.calculators.mixing import AverageCalculator from ase.md.velocitydistribution import MaxwellBoltzmannDistribution from ase.md.nptberendsen import NPTBerendsen setup = { 'ensemble': 'npt', # ensemble 'T': 373, # temperature in K 't': 100, # time in ps 'dt': 0.5, # timestep is fs 'taut': 100, # thermostat damping in steps 'taup': 1000, # barastat dampling in steps 'log-every': 5, # log interval in steps 'pressure': 1, # pressure in bar 'compressibility': 4.57e-4 # compressibility in bar^{-1} } flags = { k: v for k,v in re.findall('--(.*?)[\\\\s,\\\\=]([^\\\\s]*)', \"$flags\") } setup.update(flags) ensemble=setup['ensemble'] T=float(setup['T']) t=float(setup['t'])*units.fs*1e3 dt=float(setup['dt'])*units.fs taut=int(setup['taut']) taup=int(setup['taup']) every=int(setup['log-every']) pressure=float(setup['pressure']) compressibility=float(setup['compressibility']) ${(model instanceof Path) ? \"calc = pinn.get_calc('$model')\" : \"\"\" models = [ \"${model.join('\" , \"')}\" ] calcs = [ pinn . get_calc ( model ) for model in models ] calc = AverageCalculator ( calcs ) \"\"\"} atoms = read(\"$init\") atoms.set_calculator(calc) MaxwellBoltzmannDistribution(atoms, T*units.kB) if ensemble == 'npt': dyn = NPTBerendsen(atoms, timestep=dt, temperature=T, pressure=pressure, taut=dt * taut, taup=dt * taup, compressibility=compressibility) if ensemble == 'nvt': dyn = NVTBerendsen(atoms, timestep=dt, temperature=T, taut=dt * taut) dyn.attach( MDLogger(dyn, atoms, 'asemd.log',stress=True, mode=\"w\"), interval=int(every)) dyn.attach( Trajectory('asemd.traj', 'w', atoms).write, interval=int(every)) dyn.run(int(t/dt)) \"\"\" }","title":"ASE"},{"location":"module/ase/#ase-workflows","text":"","title":"ASE workflows"},{"location":"module/ase/#asemd","text":"The aseMD process takes a trained model and runs a MD trajecotry, with some limited options regarding the dynamics. The process suppors PiNN models only for now. A list of models can be supplied to run an ensemble MD. For more complex processes, consider writing a customized MD process and include it in your workflow.","title":"aseMD"},{"location":"module/ase/#channels","text":"Channel Type Note (in) name val an id to identify the process (in) model file trained ANN model, can be a list of models (in) flags val a string specifying the MD simulation see options below (in) init file initial geometry, in any ASE recognizable format (in) pub val path to publish the output trajectory (out) traj file output trajectory (out) log file MD log","title":"Channels"},{"location":"module/ase/#options","text":"The MD specifics for aseMD is specified as a string of flags in the form of --option1 val1 --option2 val , the available options and default valeus are down below. Option Default Note --ensemble 'nvt' 'npt' or 'nvt' --T 273 Temperature in K --t 100 Time in ps --dt 0.5 Time step in fs --taut 100 Damping factor for thermostat in steps --taup 1000 Damping factor for barostat in steps --log-every 5 Log interval in steps --pressure 1 pressure in bar --compressibility 4.57e-5 compressibility in bar Source code nextflow . enable . dsl = 2 params . publish = 'ase' process aseMD { tag \"$name\" label 'ase' publishDir \"$param.publish/$name\" input: val name path model , stageAs: 'model*' path init val flags output: tuple val ( name ), path ( 'asemd.traj' ), emit: traj tuple val ( name ), path ( 'asemd.log' ), emit: log script: \"\"\" #!/usr/bin/env python import re import pinn import tensorflow as tf from ase import units from ase.io import read from ase.io.trajectory import Trajectory from ase.md import MDLogger from ase.calculators.mixing import AverageCalculator from ase.md.velocitydistribution import MaxwellBoltzmannDistribution from ase.md.nptberendsen import NPTBerendsen setup = { 'ensemble': 'npt', # ensemble 'T': 373, # temperature in K 't': 100, # time in ps 'dt': 0.5, # timestep is fs 'taut': 100, # thermostat damping in steps 'taup': 1000, # barastat dampling in steps 'log-every': 5, # log interval in steps 'pressure': 1, # pressure in bar 'compressibility': 4.57e-4 # compressibility in bar^{-1} } flags = { k: v for k,v in re.findall('--(.*?)[\\\\s,\\\\=]([^\\\\s]*)', \"$flags\") } setup.update(flags) ensemble=setup['ensemble'] T=float(setup['T']) t=float(setup['t'])*units.fs*1e3 dt=float(setup['dt'])*units.fs taut=int(setup['taut']) taup=int(setup['taup']) every=int(setup['log-every']) pressure=float(setup['pressure']) compressibility=float(setup['compressibility']) ${(model instanceof Path) ? \"calc = pinn.get_calc('$model')\" : \"\"\" models = [ \"${model.join('\" , \"')}\" ] calcs = [ pinn . get_calc ( model ) for model in models ] calc = AverageCalculator ( calcs ) \"\"\"} atoms = read(\"$init\") atoms.set_calculator(calc) MaxwellBoltzmannDistribution(atoms, T*units.kB) if ensemble == 'npt': dyn = NPTBerendsen(atoms, timestep=dt, temperature=T, pressure=pressure, taut=dt * taut, taup=dt * taup, compressibility=compressibility) if ensemble == 'nvt': dyn = NVTBerendsen(atoms, timestep=dt, temperature=T, taut=dt * taut) dyn.attach( MDLogger(dyn, atoms, 'asemd.log',stress=True, mode=\"w\"), interval=int(every)) dyn.attach( Trajectory('asemd.traj', 'w', atoms).write, interval=int(every)) dyn.run(int(t/dt)) \"\"\" }","title":"Options"},{"location":"module/cp2k/","text":"CP2K workflows cp2k The CP2K process runs, a CP2K MD is specified with an input file, and the set of auxiliary files. For conviniece there's also workflows like cp2kMD that takes extra channels and prepares the input files. Those processes shares two parameters that can be specified during runtime or in nextflow.config : params.cp2k_cmd : the command for invoking cp2k params.cp2k_aux : the path (with wildcards) to the potential, basis, ... Channels Channel Type Note (in) name val an id to identify the process (in) input file CP2k input file (in) aux file Auxiliary files (out) traj file Trajectory (pos, frc, ener, cell, stress) (out) log file CP2K log (out) restart file CP2K restart file cp2kMD This is a shortcut to run an CP2K MD form given initial geometry, by taking an input file. The file will be inserted to the input as the initial gemoetry. The geometry must be recognizable by tips.io . In case that a multi-frame trajectry is used, the last frame will be used. Channels Channel 1 Type Note (in) name val an id to identify the process (in) input val a CP2k input file (in) init file a geometry file, to be inserted to the input Source code nextflow . enable . dsl = 2 params . publish = 'cp2k' params . cp2k_cmd = 'cp2k' params . cp2k_aux = null process cp2k { tag \"$name\" label 'cp2k' publishDir \"$params.publish/$name\" input: val name path input path aux output: tuple val ( name ), path ( '*.{ener,xyz,stress}' ), emit: traj tuple val ( name ), path ( 'cp2k.log' ), emit: logs tuple val ( name ), path ( '*.restart' ), emit: restart , optional: true script: \"\"\" #!/bin/bash $params.cp2k_cmd -i $input | tee cp2k.log \"\"\" } process cp2kGenInp { tag \"$name\" label 'tips' publishDir \"$params.publish/$name\" input: val name path input , stageAs: 'cp2k_skel.inp' path init val flags output: tuple val ( name ), path ( 'cp2k.inp' ) script: \"\"\" #!/usr/bin/env python import re from ase.data import chemical_symbols as symbol from tips.io import load_ds # read flags setup = { 'emap': None, 'idx': -1, 'fmt': 'auto', } flags = { k: v for k,v in re.findall('--(.*?)[\\\\s,\\\\=]([^\\\\s]*)', \"$flags\") } setup.update(flags) # load the desired geometry ds = load_ds(\"$init\", fmt=flags['fmt']) if setup['emap'] is not None: ds = ds.map_elems(setup['emap']) datum = ds[int(setup['idx'])] # edit the input file coord = [f' {symbol[e]} {x} {y} {z}' for e, (x,y,z) in zip(datum['elem'], datum['coord'])] cell = [f' {v} {x} {y} {z}' for v, (x, y, z) in zip('ABC', datum['cell'])] subsys = ['&COORD']+coord+['&END COORD']+['&CELL']+cell+['&END CELL'] lines = open(\"$input\").readlines() for idx, line in enumerate(lines): if '&END SUBSYS' in line: indent = len(line) - len(line.lstrip()) break subsys = [' '*(indent+2) + l + '\\\\n' for l in subsys] lines = lines[:idx] + subsys + lines[idx:] with open('cp2k.inp', 'w') as f: f.writelines(lines) \"\"\" } workflow cp2kMD { take: name input init flags main: ch_inp = cp2kGenInp ( name , input , init , flags ) ch_inp . multiMap { name , inp -> name: name inp: inp aux: file ( params . cp2k_aux ) } . set { ch } out = cp2k ( ch . name , ch . inp , ch . aux ) emit: traj = out . traj logs = out . logs restart = out . restart } Output channels are the same as cp2k . \u21a9","title":"CP2K"},{"location":"module/cp2k/#cp2k-workflows","text":"","title":"CP2K workflows"},{"location":"module/cp2k/#cp2k","text":"The CP2K process runs, a CP2K MD is specified with an input file, and the set of auxiliary files. For conviniece there's also workflows like cp2kMD that takes extra channels and prepares the input files. Those processes shares two parameters that can be specified during runtime or in nextflow.config : params.cp2k_cmd : the command for invoking cp2k params.cp2k_aux : the path (with wildcards) to the potential, basis, ...","title":"cp2k"},{"location":"module/cp2k/#channels","text":"Channel Type Note (in) name val an id to identify the process (in) input file CP2k input file (in) aux file Auxiliary files (out) traj file Trajectory (pos, frc, ener, cell, stress) (out) log file CP2K log (out) restart file CP2K restart file","title":"Channels"},{"location":"module/cp2k/#cp2kmd","text":"This is a shortcut to run an CP2K MD form given initial geometry, by taking an input file. The file will be inserted to the input as the initial gemoetry. The geometry must be recognizable by tips.io . In case that a multi-frame trajectry is used, the last frame will be used.","title":"cp2kMD"},{"location":"module/cp2k/#channels_1","text":"Channel 1 Type Note (in) name val an id to identify the process (in) input val a CP2k input file (in) init file a geometry file, to be inserted to the input Source code nextflow . enable . dsl = 2 params . publish = 'cp2k' params . cp2k_cmd = 'cp2k' params . cp2k_aux = null process cp2k { tag \"$name\" label 'cp2k' publishDir \"$params.publish/$name\" input: val name path input path aux output: tuple val ( name ), path ( '*.{ener,xyz,stress}' ), emit: traj tuple val ( name ), path ( 'cp2k.log' ), emit: logs tuple val ( name ), path ( '*.restart' ), emit: restart , optional: true script: \"\"\" #!/bin/bash $params.cp2k_cmd -i $input | tee cp2k.log \"\"\" } process cp2kGenInp { tag \"$name\" label 'tips' publishDir \"$params.publish/$name\" input: val name path input , stageAs: 'cp2k_skel.inp' path init val flags output: tuple val ( name ), path ( 'cp2k.inp' ) script: \"\"\" #!/usr/bin/env python import re from ase.data import chemical_symbols as symbol from tips.io import load_ds # read flags setup = { 'emap': None, 'idx': -1, 'fmt': 'auto', } flags = { k: v for k,v in re.findall('--(.*?)[\\\\s,\\\\=]([^\\\\s]*)', \"$flags\") } setup.update(flags) # load the desired geometry ds = load_ds(\"$init\", fmt=flags['fmt']) if setup['emap'] is not None: ds = ds.map_elems(setup['emap']) datum = ds[int(setup['idx'])] # edit the input file coord = [f' {symbol[e]} {x} {y} {z}' for e, (x,y,z) in zip(datum['elem'], datum['coord'])] cell = [f' {v} {x} {y} {z}' for v, (x, y, z) in zip('ABC', datum['cell'])] subsys = ['&COORD']+coord+['&END COORD']+['&CELL']+cell+['&END CELL'] lines = open(\"$input\").readlines() for idx, line in enumerate(lines): if '&END SUBSYS' in line: indent = len(line) - len(line.lstrip()) break subsys = [' '*(indent+2) + l + '\\\\n' for l in subsys] lines = lines[:idx] + subsys + lines[idx:] with open('cp2k.inp', 'w') as f: f.writelines(lines) \"\"\" } workflow cp2kMD { take: name input init flags main: ch_inp = cp2kGenInp ( name , input , init , flags ) ch_inp . multiMap { name , inp -> name: name inp: inp aux: file ( params . cp2k_aux ) } . set { ch } out = cp2k ( ch . name , ch . inp , ch . aux ) emit: traj = out . traj logs = out . logs restart = out . restart } Output channels are the same as cp2k . \u21a9","title":"Channels"},{"location":"module/lammps/","text":"LAMMPS workflows lammps The CP2K process runs, a CP2K MD is specified with an input file, and the set of auxiliary files. The process takes the params.lmp_cmd to specify the lammps binary, which can be specified during runtime or in nextflow.config . Channels Channel Type Note (in) name val an id to identify the process (in) input file LAMMPS input file (in) aux file Auxiliary files (force field, data, etc.) (out) traj file Trajectory in .dump format (out) log file LAMMPS log (out) restart file LAMMPS restart files in .restart Source code nextflow . enable . dsl = 2 params . lmp_cmd = 'lmp' params . publish = 'lmp' process lammpsMD { tag \"$name\" publishDir \"$params.publish/$name\" label 'lammps' input: val name path input path aux output: tuple val ( name ), path ( '*.dump' ), emit: traj tuple val ( name ), path ( 'log.lammps' ), emit: logs script: \"\"\" #!/bin/bash $params.lmp_cmd -i $input \"\"\" }","title":"LAMMPS"},{"location":"module/lammps/#lammps-workflows","text":"","title":"LAMMPS workflows"},{"location":"module/lammps/#lammps","text":"The CP2K process runs, a CP2K MD is specified with an input file, and the set of auxiliary files. The process takes the params.lmp_cmd to specify the lammps binary, which can be specified during runtime or in nextflow.config .","title":"lammps"},{"location":"module/lammps/#channels","text":"Channel Type Note (in) name val an id to identify the process (in) input file LAMMPS input file (in) aux file Auxiliary files (force field, data, etc.) (out) traj file Trajectory in .dump format (out) log file LAMMPS log (out) restart file LAMMPS restart files in .restart Source code nextflow . enable . dsl = 2 params . lmp_cmd = 'lmp' params . publish = 'lmp' process lammpsMD { tag \"$name\" publishDir \"$params.publish/$name\" label 'lammps' input: val name path input path aux output: tuple val ( name ), path ( '*.dump' ), emit: traj tuple val ( name ), path ( 'log.lammps' ), emit: logs script: \"\"\" #!/bin/bash $params.lmp_cmd -i $input \"\"\" }","title":"Channels"},{"location":"module/pinn/","text":"PiNN workflows pinnTrain The pinnTrain process runs a training given a dataset and a PiNN input file. The input file can optionally be a model folder in such case the process contines training from the last checkpoint available. For list of flags available, see the PiNN documentation . Channels Channel Type Note (in) name val an id to identify the process (in) dataset file a dataset recognizable by PiNN (in) input file a PiNN .yml input file (in) flag val flags for pinn train (out) model file Trained PiNN model (out) log file PiNN log (the evaluation errors) Source code nextflow . enable . dsl = 2 params . publish = 'pinn' process pinnTrain { tag \"$name\" label 'pinn' publishDir \"$params.publish/$name\" input: val name path dataset path input val flags output: tuple val ( name ), path ( 'model/' , type: 'dir' ), emit: model tuple val ( name ), path ( 'pinn.log' ), emit: log script: \"\"\" #!/bin/bash convert_flag=\"${(flags =~ /--seed[\\s,\\=]\\d+/)[0]}\" train_flags=\"${flags.replaceAll(/--seed[\\s,\\=]\\d+/, '')}\" pinn convert $dataset -o 'train:8,eval:2' \\$convert_flag if [ ! -f $input/params.yml ]; then mkdir -p model; cp $input model/params.yml else cp -rL $input model fi pinn train model/params.yml --model-dir='model'\\ --train-ds='train.yml' --eval-ds='eval.yml'\\ \\$train_flags pinn log model/eval > pinn.log \"\"\" }","title":"PiNN"},{"location":"module/pinn/#pinn-workflows","text":"","title":"PiNN workflows"},{"location":"module/pinn/#pinntrain","text":"The pinnTrain process runs a training given a dataset and a PiNN input file. The input file can optionally be a model folder in such case the process contines training from the last checkpoint available. For list of flags available, see the PiNN documentation .","title":"pinnTrain"},{"location":"module/pinn/#channels","text":"Channel Type Note (in) name val an id to identify the process (in) dataset file a dataset recognizable by PiNN (in) input file a PiNN .yml input file (in) flag val flags for pinn train (out) model file Trained PiNN model (out) log file PiNN log (the evaluation errors) Source code nextflow . enable . dsl = 2 params . publish = 'pinn' process pinnTrain { tag \"$name\" label 'pinn' publishDir \"$params.publish/$name\" input: val name path dataset path input val flags output: tuple val ( name ), path ( 'model/' , type: 'dir' ), emit: model tuple val ( name ), path ( 'pinn.log' ), emit: log script: \"\"\" #!/bin/bash convert_flag=\"${(flags =~ /--seed[\\s,\\=]\\d+/)[0]}\" train_flags=\"${flags.replaceAll(/--seed[\\s,\\=]\\d+/, '')}\" pinn convert $dataset -o 'train:8,eval:2' \\$convert_flag if [ ! -f $input/params.yml ]; then mkdir -p model; cp $input model/params.yml else cp -rL $input model fi pinn train model/params.yml --model-dir='model'\\ --train-ds='train.yml' --eval-ds='eval.yml'\\ \\$train_flags pinn log model/eval > pinn.log \"\"\" }","title":"Channels"},{"location":"module/utils/","text":"Utility processes convert The convert process converts one dataset to another, where the dataset input can be a list of files (in such case), the dataset will be joined together. Channel Type Note (in) name val an id to identify the process (in) dataset file input dataset(s) (in) flags val tips convert options (out) output file the converted dataset","title":"Utils"},{"location":"module/utils/#utility-processes","text":"","title":"Utility processes"},{"location":"module/utils/#convert","text":"The convert process converts one dataset to another, where the dataset input can be a list of files (in such case), the dataset will be joined together. Channel Type Note (in) name val an id to identify the process (in) dataset file input dataset(s) (in) flags val tips convert options (out) output file the converted dataset","title":"convert"},{"location":"python/convert/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"tips.bias"},{"location":"python/convert/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"python/io/","text":"The TIPS IO module The IO module in TIPS allows for the translation between different atomistic data formats, with a special focus for AML. The core class is tips.io.Dataset , which holds the dataset and its metadata, and allows for manipulation and convertion of the data. To create datasets, the easiest way is to use the universal data loader load_ds : Usage Output from tips.io import load_ds ds = load_ds ( 'path/to/dataset' , fmt = deepmd - raw ') ds = ds . join ( ds ) # datasets can be joined together ds . convert ( 'dataset.yml' , fmt = 'pinn' ) # the and converted to different formats print ( ds ) # printing the dataset shows basic information about the dataset < tips . io . Dataset : fmt : DeePMD raw size : 100 elem : 8 , 1 spec : cell : 3 x3 , float elem : [ None ], int force : [ None , 3 ], float coord : [ None , 3 ], float energy : [], float > Detailed descriptions about the Dataset object can be found in its API documentation . Available formats Check: implemented; No-entry: not implemented yet; Empty: not-planned Format Read Convert Note ase ASE Atoms objects cp2k CP2K data (pos, frc, and cell) deepmd DeePMD format ext-xyz Extended XYZ format lammps LAMMPS dump format pinn PiNN-style TFRecord format runner RuNNer format Custom reader/writer It is possible to extend TIPS by registering extra reader/writers, an example for custom reader/writer can be found below: Example implementation of custom data reader and converters from tips.io.utils import tips_reader , tips_convert @tips_reader ( 'my-ase' ) def load_ase ( traj ): \"\"\" An example reader for ASE Atoms The function should return a tips Dataset, by specifying at least an generator which yields elements in the dataset one by one, and the metadata specifying the data structure. (the generator is redundent ni the below case because an ASE trajectory is indexable and has a defined size, such a generator will be defined automatically by tips) Args: traj: list of atoms Returns: tips.io.Dataset \"\"\" from tips.io import Dataset meta = { 'spec' : { 'elems' : { 'shape' : [ None ], 'dtype' : 'int32' }, 'coord' : { 'shape' : [ None , 3 ], 'dtype' : 'float32' }, 'cell' : { 'shape' : [ 3 , 3 ], 'dtype' : 'float32' } }, 'size' : len ( traj ), 'fmt' : 'Custom ASE Format' } def indexer ( i ): atoms = traj [ i ] data = { 'elems' : atoms . numbers 'coord' : atoms . positions , 'cell' : atoms . cell , } return data def generator (): for i in range ( meta [ 'size' ]): yield indexer ( i ) return Dataset ( generator = generator , meta = meta , indexer = indexer ) @tips_convert ( 'my-ase' ) def ds_to_ase ( dataset ): \"\"\" An example data converter to ASE trajectory The function must takes on dataset and optionally extra keyword arguments as inputs. There is no limitaton on the return values. Args: dataset (tips.io.Dataset): a dataset object \"\"\" from ase import Atoms traj = [ Atoms ( data [ 'elems' ], positions = data [ 'coord' ], cell = data [ 'cell' ]) for data in dataset ] return traj The additonal format will be available for data loading and conversion: ds = load_ds ([ Atoms [ 'H' ], Atoms [ 'Cu' ]], fmt = 'my-ase' ) traj = ds . convert ( fmt = 'my-ase' ) Registered datasets TIPS curates a small list of datasets that can be directly accessed via the load_ds function. For now, the list exist mainly for test and demonstrative purpose. Not Implemented yet! API documentation Not Implemented yet!","title":"tips.io"},{"location":"python/io/#the-tips-io-module","text":"The IO module in TIPS allows for the translation between different atomistic data formats, with a special focus for AML. The core class is tips.io.Dataset , which holds the dataset and its metadata, and allows for manipulation and convertion of the data. To create datasets, the easiest way is to use the universal data loader load_ds : Usage Output from tips.io import load_ds ds = load_ds ( 'path/to/dataset' , fmt = deepmd - raw ') ds = ds . join ( ds ) # datasets can be joined together ds . convert ( 'dataset.yml' , fmt = 'pinn' ) # the and converted to different formats print ( ds ) # printing the dataset shows basic information about the dataset < tips . io . Dataset : fmt : DeePMD raw size : 100 elem : 8 , 1 spec : cell : 3 x3 , float elem : [ None ], int force : [ None , 3 ], float coord : [ None , 3 ], float energy : [], float > Detailed descriptions about the Dataset object can be found in its API documentation .","title":"The TIPS IO module"},{"location":"python/io/#available-formats","text":"Check: implemented; No-entry: not implemented yet; Empty: not-planned Format Read Convert Note ase ASE Atoms objects cp2k CP2K data (pos, frc, and cell) deepmd DeePMD format ext-xyz Extended XYZ format lammps LAMMPS dump format pinn PiNN-style TFRecord format runner RuNNer format","title":"Available formats"},{"location":"python/io/#custom-readerwriter","text":"It is possible to extend TIPS by registering extra reader/writers, an example for custom reader/writer can be found below: Example implementation of custom data reader and converters from tips.io.utils import tips_reader , tips_convert @tips_reader ( 'my-ase' ) def load_ase ( traj ): \"\"\" An example reader for ASE Atoms The function should return a tips Dataset, by specifying at least an generator which yields elements in the dataset one by one, and the metadata specifying the data structure. (the generator is redundent ni the below case because an ASE trajectory is indexable and has a defined size, such a generator will be defined automatically by tips) Args: traj: list of atoms Returns: tips.io.Dataset \"\"\" from tips.io import Dataset meta = { 'spec' : { 'elems' : { 'shape' : [ None ], 'dtype' : 'int32' }, 'coord' : { 'shape' : [ None , 3 ], 'dtype' : 'float32' }, 'cell' : { 'shape' : [ 3 , 3 ], 'dtype' : 'float32' } }, 'size' : len ( traj ), 'fmt' : 'Custom ASE Format' } def indexer ( i ): atoms = traj [ i ] data = { 'elems' : atoms . numbers 'coord' : atoms . positions , 'cell' : atoms . cell , } return data def generator (): for i in range ( meta [ 'size' ]): yield indexer ( i ) return Dataset ( generator = generator , meta = meta , indexer = indexer ) @tips_convert ( 'my-ase' ) def ds_to_ase ( dataset ): \"\"\" An example data converter to ASE trajectory The function must takes on dataset and optionally extra keyword arguments as inputs. There is no limitaton on the return values. Args: dataset (tips.io.Dataset): a dataset object \"\"\" from ase import Atoms traj = [ Atoms ( data [ 'elems' ], positions = data [ 'coord' ], cell = data [ 'cell' ]) for data in dataset ] return traj The additonal format will be available for data loading and conversion: ds = load_ds ([ Atoms [ 'H' ], Atoms [ 'Cu' ]], fmt = 'my-ase' ) traj = ds . convert ( fmt = 'my-ase' )","title":"Custom reader/writer"},{"location":"python/io/#registered-datasets","text":"TIPS curates a small list of datasets that can be directly accessed via the load_ds function. For now, the list exist mainly for test and demonstrative purpose. Not Implemented yet!","title":"Registered datasets"},{"location":"python/io/#api-documentation","text":"Not Implemented yet!","title":"API documentation"},{"location":"python/reduction/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"tips.reduction"},{"location":"python/reduction/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"python/subsample/","text":"Documentation under construction Warning This is a placeholder for future documentation.","title":"tips.subsample"},{"location":"python/subsample/#documentation-under-construction","text":"Warning This is a placeholder for future documentation.","title":"Documentation under construction"},{"location":"recipe/active/","text":"Active workflows Usage The below commands generate a workflow in main.nf and then runs it via nextflow. The TIPS wizard generates a generic skeleton for designing the workflows, to futher tweak it, please refer to the annotated main.nf and nextflow.config files. commands main.nf nextflow.config tips wizard active nextflow run main.nf #!/usr/bin/env nextflow params . dataset = 'qm9' params . input = './inputs/*.yml' params . md_init = 'h2o.xyz' params . md_flag = '--nvt --T 373 --step 1000 --dt 0.5' // to be written profiles { standard { process { cpus=1 errorStrategy='ignore' withLabel: pinn {container='yqshao/pinn:master'} withLabel: tame {container='yqshao/tame:master'} } executor { name = 'local' cpus = 16 } } Strategies TIPS provides different strategies for an active leraning task, which affects the efficiency and the resulting NN. ENN : In an ENN (ensemble NN) workflow, the trajectory is propogated with an ensemble of NN models, while a subset of the trajectory is labelled according to a given uncertainty tolerance. DAS : The DAS (density-based adaptive sampling) scheme samples the configuration space by actively biasing the dynamics according to the data distribution in latent space. Other links Sample model input files Adding a custom analysis","title":"Active learning"},{"location":"recipe/active/#active-workflows","text":"","title":"Active workflows"},{"location":"recipe/active/#usage","text":"The below commands generate a workflow in main.nf and then runs it via nextflow. The TIPS wizard generates a generic skeleton for designing the workflows, to futher tweak it, please refer to the annotated main.nf and nextflow.config files. commands main.nf nextflow.config tips wizard active nextflow run main.nf #!/usr/bin/env nextflow params . dataset = 'qm9' params . input = './inputs/*.yml' params . md_init = 'h2o.xyz' params . md_flag = '--nvt --T 373 --step 1000 --dt 0.5' // to be written profiles { standard { process { cpus=1 errorStrategy='ignore' withLabel: pinn {container='yqshao/pinn:master'} withLabel: tame {container='yqshao/tame:master'} } executor { name = 'local' cpus = 16 } }","title":"Usage"},{"location":"recipe/active/#strategies","text":"TIPS provides different strategies for an active leraning task, which affects the efficiency and the resulting NN. ENN : In an ENN (ensemble NN) workflow, the trajectory is propogated with an ensemble of NN models, while a subset of the trajectory is labelled according to a given uncertainty tolerance. DAS : The DAS (density-based adaptive sampling) scheme samples the configuration space by actively biasing the dynamics according to the data distribution in latent space.","title":"Strategies"},{"location":"recipe/active/#other-links","text":"Sample model input files Adding a custom analysis","title":"Other links"},{"location":"recipe/benchmark/","text":"Benchmark workflows Usage The below commands generate a workflow in main.nf and then runs it via nextflow. The TIPS wizard generates a generic skeleton for designing the workflows, to futher tweak it, please refer to the annotated main.nf and nextflow.config files. commands main.nf nextflow.config tips wizard benchmark nextflow run main.nf #!/usr/bin/env nextflow params . dataset = 'qm9' params . input = './inputs/*.yml' params . md_init = 'h2o.xyz' params . md_flag = '--nvt --T 373 --step 1000 --dt 0.5' params . rdf_flag = '--tags OO,OH --rc 5' include { pinnTrain } from './pinn.nf' include { aseMD } from './ase.nf' include { rdf } from './analysis.nf' workflow { dataset = Channel . fromPath ( params . dataset ) input = Channel . fromPath ( params . input ) models = pinnTrain ( dataset , input ) trajs = aseMD ( models ) rdf ( trajs , rdf_tags ) } profiles { standard { process { cpus = 1 errorStrategy = 'ignore' withLabel: pinn { container = 'yqshao/pinn:master' } withLabel: tame { container = 'yqshao/tame:master' } } executor { name = 'local' cpus = 16 } } Other links List of available datasets Sample model input files Adding a custom analysis","title":"Benchmarking"},{"location":"recipe/benchmark/#benchmark-workflows","text":"","title":"Benchmark workflows"},{"location":"recipe/benchmark/#usage","text":"The below commands generate a workflow in main.nf and then runs it via nextflow. The TIPS wizard generates a generic skeleton for designing the workflows, to futher tweak it, please refer to the annotated main.nf and nextflow.config files. commands main.nf nextflow.config tips wizard benchmark nextflow run main.nf #!/usr/bin/env nextflow params . dataset = 'qm9' params . input = './inputs/*.yml' params . md_init = 'h2o.xyz' params . md_flag = '--nvt --T 373 --step 1000 --dt 0.5' params . rdf_flag = '--tags OO,OH --rc 5' include { pinnTrain } from './pinn.nf' include { aseMD } from './ase.nf' include { rdf } from './analysis.nf' workflow { dataset = Channel . fromPath ( params . dataset ) input = Channel . fromPath ( params . input ) models = pinnTrain ( dataset , input ) trajs = aseMD ( models ) rdf ( trajs , rdf_tags ) } profiles { standard { process { cpus = 1 errorStrategy = 'ignore' withLabel: pinn { container = 'yqshao/pinn:master' } withLabel: tame { container = 'yqshao/tame:master' } } executor { name = 'local' cpus = 16 } }","title":"Usage"},{"location":"recipe/benchmark/#other-links","text":"List of available datasets Sample model input files Adding a custom analysis","title":"Other links"},{"location":"recipe/overview/","text":"Overview You see from the tutorial how a workflow looks like and how it runs. While it is handy to use one the wizards to generate your workflow, it is often desirable to modify it to suit your specific need. This part of the documentation hosts detailed documentation of the workflows generated by the wizard (recipes) and their components (modules), to help you understand how the workflow in TIPS are designed.","title":"Overview"},{"location":"recipe/overview/#overview","text":"You see from the tutorial how a workflow looks like and how it runs. While it is handy to use one the wizards to generate your workflow, it is often desirable to modify it to suit your specific need. This part of the documentation hosts detailed documentation of the workflows generated by the wizard (recipes) and their components (modules), to help you understand how the workflow in TIPS are designed.","title":"Overview"},{"location":"start/alternatives/","text":"Alternatives Below lists known implementations of active atomistic machine learning codes that shares some purposes with TIPS (benchmarking MLPs or active learning with MLPs). If you are not sure about TIPS, maybe it will help you to evaluate the alternatives. OpenMM For classical force fields, OpenMM is a library https://openmm.org/ecosystem DP-GEN DP-GEN is possibly one of the earliest open-source code that automates the generation of MLPs. The code is mainly tightly integrated with the DeepPotential, and interfaces to a wide variety of QM packages ( official site , github repo ) MLAtom MLAtom is interfaced to several third-party AML libraries that allows for the benchmark of AML methods. ( official site ) Atomistic Adversal attack This is a demo project based on the Neural Force Field (NFF) code, featuring the active sampling of sampling of molecular geometries with adveral attack. ( github repo ) Related lists Composing reusable workflows is a common problem in different branches of computational sciences. Specifically for atomistic simulations, much effort was taken to bridge and interface different softwares. Those tools is better summerized in other lists provied below: Awesome workflow engines","title":"Alternatives"},{"location":"start/alternatives/#alternatives","text":"Below lists known implementations of active atomistic machine learning codes that shares some purposes with TIPS (benchmarking MLPs or active learning with MLPs). If you are not sure about TIPS, maybe it will help you to evaluate the alternatives.","title":"Alternatives"},{"location":"start/alternatives/#openmm","text":"For classical force fields, OpenMM is a library https://openmm.org/ecosystem","title":"OpenMM"},{"location":"start/alternatives/#dp-gen","text":"DP-GEN is possibly one of the earliest open-source code that automates the generation of MLPs. The code is mainly tightly integrated with the DeepPotential, and interfaces to a wide variety of QM packages ( official site , github repo )","title":"DP-GEN"},{"location":"start/alternatives/#mlatom","text":"MLAtom is interfaced to several third-party AML libraries that allows for the benchmark of AML methods. ( official site )","title":"MLAtom"},{"location":"start/alternatives/#atomistic-adversal-attack","text":"This is a demo project based on the Neural Force Field (NFF) code, featuring the active sampling of sampling of molecular geometries with adveral attack. ( github repo )","title":"Atomistic Adversal attack"},{"location":"start/alternatives/#related-lists","text":"Composing reusable workflows is a common problem in different branches of computational sciences. Specifically for atomistic simulations, much effort was taken to bridge and interface different softwares. Those tools is better summerized in other lists provied below: Awesome workflow engines","title":"Related lists"},{"location":"start/configure/","text":"Configure the workflow The nextflow config file In your workflow folder, you will find a file named nextflow.config that looks like this: Singularity Slurm profiles { standard { params { lmp_cmd = 'mpirun -np ${task.cpus} lmp_mpi' cp2k_cmd = 'mpirun -np ${task.cpus} cp2k.popt' } process { errorStrategy = 'ignore' withLabel: tips { container = 'yqshao/tips:tips-latest' } withLabel: pinn { container = 'yqshao/tips:pinn-latest' } withLabel: cp2k { container = 'yqshao/tips:cp2k-latest' } withLabel: utils { container = 'yqshao/tips:utils-latest' } withLabel: lammps { container = 'yqshao/tips:lammps-latest' }} executor { name = 'local' cpus = 4 }}} singularity { enabled = true autoMounts = true } profiles { standard { params { lmp_cmd = 'mpirun -np ${task.cpus} lmp_mpi' cp2k_cmd = 'mpirun -np ${task.cpus} cp2k.popt' } process { errorStrategy = 'ignore' withLabel: tips { container = 'yqshao/tips:tips-latest' } withLabel: pinn { container = 'yqshao/tips:pinn-latest' } withLabel: cp2k { container = 'yqshao/tips:cp2k-latest' } withLabel: utils { container = 'yqshao/tips:utils-latest' } withLabel: lammps { container = 'yqshao/tips:lammps-latest' }} executor { name = 'local' cpus = 4 }}} singularity { enabled = true autoMounts = true } The file specifies details about your computational resources that is independent of the workflow, e.g., the executable for your package, queuing system, the resource you wish to use, etc. Adding multiple profiles In the above example, the config is written to the \"standard\" profile. If you would like to share the same workflow across different computational resources, you can add additional profiles to the config, and switch them using the -profile argument when running, i.e.: Command nextflow run main.nf -profile my_profile Check the nextflow documentation for some examples. General recommandation When the project is generated with tips wizard , a minimal configuration file is automatically generated as in the above example. The default provided by TIPS might not fit your need. For instance, you might prefer the binaries compiled by your local HPC cluster than the singularity images, or you might want to allocate different resources to different types of calculations. TIPS curates a list of profiles for some of the computational resources we have access to. Those can be good starting points for you to adapt for your own need. You are also welcome to contribute your config if think it will be useful for others.","title":"Configure your run"},{"location":"start/configure/#configure-the-workflow","text":"","title":"Configure the workflow"},{"location":"start/configure/#the-nextflow-config-file","text":"In your workflow folder, you will find a file named nextflow.config that looks like this: Singularity Slurm profiles { standard { params { lmp_cmd = 'mpirun -np ${task.cpus} lmp_mpi' cp2k_cmd = 'mpirun -np ${task.cpus} cp2k.popt' } process { errorStrategy = 'ignore' withLabel: tips { container = 'yqshao/tips:tips-latest' } withLabel: pinn { container = 'yqshao/tips:pinn-latest' } withLabel: cp2k { container = 'yqshao/tips:cp2k-latest' } withLabel: utils { container = 'yqshao/tips:utils-latest' } withLabel: lammps { container = 'yqshao/tips:lammps-latest' }} executor { name = 'local' cpus = 4 }}} singularity { enabled = true autoMounts = true } profiles { standard { params { lmp_cmd = 'mpirun -np ${task.cpus} lmp_mpi' cp2k_cmd = 'mpirun -np ${task.cpus} cp2k.popt' } process { errorStrategy = 'ignore' withLabel: tips { container = 'yqshao/tips:tips-latest' } withLabel: pinn { container = 'yqshao/tips:pinn-latest' } withLabel: cp2k { container = 'yqshao/tips:cp2k-latest' } withLabel: utils { container = 'yqshao/tips:utils-latest' } withLabel: lammps { container = 'yqshao/tips:lammps-latest' }} executor { name = 'local' cpus = 4 }}} singularity { enabled = true autoMounts = true } The file specifies details about your computational resources that is independent of the workflow, e.g., the executable for your package, queuing system, the resource you wish to use, etc.","title":"The nextflow config file"},{"location":"start/configure/#adding-multiple-profiles","text":"In the above example, the config is written to the \"standard\" profile. If you would like to share the same workflow across different computational resources, you can add additional profiles to the config, and switch them using the -profile argument when running, i.e.: Command nextflow run main.nf -profile my_profile Check the nextflow documentation for some examples.","title":"Adding multiple profiles"},{"location":"start/configure/#general-recommandation","text":"When the project is generated with tips wizard , a minimal configuration file is automatically generated as in the above example. The default provided by TIPS might not fit your need. For instance, you might prefer the binaries compiled by your local HPC cluster than the singularity images, or you might want to allocate different resources to different types of calculations. TIPS curates a list of profiles for some of the computational resources we have access to. Those can be good starting points for you to adapt for your own need. You are also welcome to contribute your config if think it will be useful for others.","title":"General recommandation"},{"location":"start/first_workflow/","text":"Your first workflow In this section, you'll run the workflow from the previous step and see what TIPS can do. Running the workflow To the workflow you get from the preview section, simply run: nextflow run main.nf if you have chosen the default config, nextflow will fetch the necessary singularity images and run the training on you local computer. If you are lucky, you will see the trained models as well as their training logs in you models folder. Structure of the project The TIPS script provides a way to run your script in a reproducible, and scalable way. With the main.nf script, you are ready to run the same workflow on any computational resource, and easily include your own tricks to fit your need. See the annotated fodler structure and files below for more explanation. Folder structure main.nf your_ | - main.nf # (1) | - nextflow.config # (2) | - tips # (3) | - pinn.nf | - work # (4) | - f3/fa4ce305c2b0d057af8b83ad9b0aec | - .... | - models # (5) | - qm9-pinet-1 | - qm9-pinet-2 | - .... This is the workflow file that you execute, see the tab on the left. This controls how the jobs are run on different resources. TIPS provides several modules for different tasks, those are stored in the tips subfolder, see the modules documentation for their specific usage. This holds the actual runtime folder of the jobs, where the scripts, data and intermediate results sites. The models (or alike) folders are \"published\" by nextflow, tips have a sysmtatic way to order the jobs, see general rules . import { pinnTrain } from 'tips/pinn.nf' // (1) params . ds = 'datasets/h2o.data' // (2) params . inps = 'inputs/pinet.yml' params . flags = '--train-steps 1000000 --batch 30' workflow { channel . fromPath ( params . ds ) . combine ( Channel . fromPath ( params . inps )) . multiMap { ds , inp: name: $ \"$ds.baseName-inp.baseName\" ds: ds inp: inp flags: params . flags } . set { ch } // (3) pinnTrain ( ch . name , ch . ds , ch . inp , ch . flags ) } This imports the pinnTrain process that does the actual training, you can find from the documentation that under the hood, it's just a bash script. To build your own workflow, feel free to substitue in your favorate ML/MD package. The params.xx lines defines variables you can change during runtime, for instance running the same script with nextflow run main --ds 'dataset/water.data' trains your model on a different dataset. TIPS build up complex workflows following a few rules. Here, we create the data channels and name the runs according to the dataset and PiNN input. The channels are built from named channels with the multiMap function, see nextflow documentation for the details. Nextflow basics Below going further, there are some nextflow tricks that you might find useful: Checking Resuming Cleaning $ nextflow log # (1) ------------------------------------ # (2) TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2022 -04-26 22 :28:35 1m 13s tender_varahamihira OK e7132a82d7 4f445e64-8d54-48dd-9fea-b57d9be3e5c9 nextflow run 2022 -04-26 22 :52:18 1h 40m 53s tiny_brattain OK e7132a82d7 bf527311-a5d9-4f7b-b615-d50ff99e6ec5 nextflow run Nextflow keeps the record of your runs with a Each run of the workflow is attached to a \"run name\", with which you can resume, log, and clean up the jobs, try nextflow log tiny_brattain (replace with your own run) to get a list of all jobs in a run. $ nextflow run main.nf -resume # (1) --- Launching ` main.nf ` [ disturbed_crick ] - revision: 3439ecc683 executor > local ( 6 ) [ 85 /e10f9e ] process > pinnTrain ( h2o-pinet-1 ) [ 50 % ] 3 of 6 , cached: 3 Nextflow caches the jobs according to their inputs, if your run is interrupted, you can resume from where your stopped with the -resume . You can also specify a previous run to resume from: nextflow run main.nf -resume tiny_brattain $ nextflow clean -f disturbed_crick # (1) --- Removed /home/user/proj/work/3f/f350726963c2372d1673b900553d6f Removed /home/user/proj/work/83/60f47f79880256257fc53a19b6ffac Removed /home/user/proj/work/5f/bd7e2a391cb53a4e20057c4641e408 Keeping a complete record of things come at a cost, you are recommended to clean up your unneeded runs regularly. There are also the -before , -after and -but to clean up multiple runs a time. For a more comprehensive description, check the Nextflow CLI reference . What next? Configure your run your run to work on any computational resource. Some more complex examples of how workflows are composed. Explore how TIPS as a python library .","title":"Your first workflow"},{"location":"start/first_workflow/#your-first-workflow","text":"In this section, you'll run the workflow from the previous step and see what TIPS can do.","title":"Your first workflow"},{"location":"start/first_workflow/#running-the-workflow","text":"To the workflow you get from the preview section, simply run: nextflow run main.nf if you have chosen the default config, nextflow will fetch the necessary singularity images and run the training on you local computer. If you are lucky, you will see the trained models as well as their training logs in you models folder.","title":"Running the workflow"},{"location":"start/first_workflow/#structure-of-the-project","text":"The TIPS script provides a way to run your script in a reproducible, and scalable way. With the main.nf script, you are ready to run the same workflow on any computational resource, and easily include your own tricks to fit your need. See the annotated fodler structure and files below for more explanation. Folder structure main.nf your_ | - main.nf # (1) | - nextflow.config # (2) | - tips # (3) | - pinn.nf | - work # (4) | - f3/fa4ce305c2b0d057af8b83ad9b0aec | - .... | - models # (5) | - qm9-pinet-1 | - qm9-pinet-2 | - .... This is the workflow file that you execute, see the tab on the left. This controls how the jobs are run on different resources. TIPS provides several modules for different tasks, those are stored in the tips subfolder, see the modules documentation for their specific usage. This holds the actual runtime folder of the jobs, where the scripts, data and intermediate results sites. The models (or alike) folders are \"published\" by nextflow, tips have a sysmtatic way to order the jobs, see general rules . import { pinnTrain } from 'tips/pinn.nf' // (1) params . ds = 'datasets/h2o.data' // (2) params . inps = 'inputs/pinet.yml' params . flags = '--train-steps 1000000 --batch 30' workflow { channel . fromPath ( params . ds ) . combine ( Channel . fromPath ( params . inps )) . multiMap { ds , inp: name: $ \"$ds.baseName-inp.baseName\" ds: ds inp: inp flags: params . flags } . set { ch } // (3) pinnTrain ( ch . name , ch . ds , ch . inp , ch . flags ) } This imports the pinnTrain process that does the actual training, you can find from the documentation that under the hood, it's just a bash script. To build your own workflow, feel free to substitue in your favorate ML/MD package. The params.xx lines defines variables you can change during runtime, for instance running the same script with nextflow run main --ds 'dataset/water.data' trains your model on a different dataset. TIPS build up complex workflows following a few rules. Here, we create the data channels and name the runs according to the dataset and PiNN input. The channels are built from named channels with the multiMap function, see nextflow documentation for the details.","title":"Structure of the project"},{"location":"start/first_workflow/#nextflow-basics","text":"Below going further, there are some nextflow tricks that you might find useful: Checking Resuming Cleaning $ nextflow log # (1) ------------------------------------ # (2) TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2022 -04-26 22 :28:35 1m 13s tender_varahamihira OK e7132a82d7 4f445e64-8d54-48dd-9fea-b57d9be3e5c9 nextflow run 2022 -04-26 22 :52:18 1h 40m 53s tiny_brattain OK e7132a82d7 bf527311-a5d9-4f7b-b615-d50ff99e6ec5 nextflow run Nextflow keeps the record of your runs with a Each run of the workflow is attached to a \"run name\", with which you can resume, log, and clean up the jobs, try nextflow log tiny_brattain (replace with your own run) to get a list of all jobs in a run. $ nextflow run main.nf -resume # (1) --- Launching ` main.nf ` [ disturbed_crick ] - revision: 3439ecc683 executor > local ( 6 ) [ 85 /e10f9e ] process > pinnTrain ( h2o-pinet-1 ) [ 50 % ] 3 of 6 , cached: 3 Nextflow caches the jobs according to their inputs, if your run is interrupted, you can resume from where your stopped with the -resume . You can also specify a previous run to resume from: nextflow run main.nf -resume tiny_brattain $ nextflow clean -f disturbed_crick # (1) --- Removed /home/user/proj/work/3f/f350726963c2372d1673b900553d6f Removed /home/user/proj/work/83/60f47f79880256257fc53a19b6ffac Removed /home/user/proj/work/5f/bd7e2a391cb53a4e20057c4641e408 Keeping a complete record of things come at a cost, you are recommended to clean up your unneeded runs regularly. There are also the -before , -after and -but to clean up multiple runs a time. For a more comprehensive description, check the Nextflow CLI reference .","title":"Nextflow basics"},{"location":"start/first_workflow/#what-next","text":"Configure your run your run to work on any computational resource. Some more complex examples of how workflows are composed. Explore how TIPS as a python library .","title":"What next?"},{"location":"start/help/","text":"Getting help If you meet any problem in using the TIPS code, you can welcome to reach us for help. Slack channel TIPS shares a discussion channel with the PiNN code: link . Issue tracker If you think there's a bug in the code, you can submit a bug report through Github. Useful links Nextflow documentation , forum and slack channel ; CP2K manual and google group ; LAMMPS documentation ; MatSci forum .","title":"Getting help"},{"location":"start/help/#getting-help","text":"If you meet any problem in using the TIPS code, you can welcome to reach us for help.","title":"Getting help"},{"location":"start/help/#slack-channel","text":"TIPS shares a discussion channel with the PiNN code: link .","title":"Slack channel"},{"location":"start/help/#issue-tracker","text":"If you think there's a bug in the code, you can submit a bug report through Github.","title":"Issue tracker"},{"location":"start/help/#useful-links","text":"Nextflow documentation , forum and slack channel ; CP2K manual and google group ; LAMMPS documentation ; MatSci forum .","title":"Useful links"},{"location":"start/install/","text":"Get started Install TIPS via pip TIPS is a chain of tools towards the construction of machine-learnt interatomic potentials. At its core, TIPS consists of a hierachy of nextflow workflows for a variety of atomistic machine learning tasks. If you are familiar with nextflow, you can run the recipes without even installing anything. For newcomers, it's recommanded to install the TIPS CLI, via the pip command: Command pip install 'git+https://github.com/yqshao/tips.git#egg=tips&subdirectory=python' First step To get started, the easiest way is to create a new project with the interactive wizard command: Command tips wizard The wizard will walk you through the configuration of your workflow and create a folder with the necessary files. To run the worflow, you'll also need to install Nextflow (the wizard will direct you to the nextflow installation guide if it is not found). The next sections will detail the contents in the folder and the execution of the workflow.","title":"Installation"},{"location":"start/install/#get-started","text":"","title":"Get started"},{"location":"start/install/#install-tips-via-pip","text":"TIPS is a chain of tools towards the construction of machine-learnt interatomic potentials. At its core, TIPS consists of a hierachy of nextflow workflows for a variety of atomistic machine learning tasks. If you are familiar with nextflow, you can run the recipes without even installing anything. For newcomers, it's recommanded to install the TIPS CLI, via the pip command: Command pip install 'git+https://github.com/yqshao/tips.git#egg=tips&subdirectory=python'","title":"Install TIPS via pip"},{"location":"start/install/#first-step","text":"To get started, the easiest way is to create a new project with the interactive wizard command: Command tips wizard The wizard will walk you through the configuration of your workflow and create a folder with the necessary files. To run the worflow, you'll also need to install Nextflow (the wizard will direct you to the nextflow installation guide if it is not found). The next sections will detail the contents in the folder and the execution of the workflow.","title":"First step"},{"location":"start/license/","text":"License BSD-3-Clause License Copyright \u00a9 2022, TIPS developers All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Teoroo-CMC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL TIPS developers BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"start/license/#license","text":"BSD-3-Clause License Copyright \u00a9 2022, TIPS developers All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Teoroo-CMC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL TIPS developers BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"start/references/","text":"References","title":"References"},{"location":"start/references/#references","text":"","title":"References"}]}